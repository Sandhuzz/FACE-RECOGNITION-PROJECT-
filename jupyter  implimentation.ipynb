{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1731324-19bc-4084-8565-91c0c392697a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Num GPUs Available: 0\n",
      "GPU Details: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Details:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640007f4-98d3-4fb7-8b35-7a558df28644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras_preprocessing.image import load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f45bb38-a947-4cf2-8f62-68f8ad17e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"C:/Users/Asus/OneDrive/Documents/face recognition project/images/train\"\n",
    "TEST_DIR = \"C:/Users/Asus/OneDrive/Documents/face recognition project/images/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e3f5644-1b12-40bf-b0cf-0e4fd1cb5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createdataframe(dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for label in os.listdir(dir):\n",
    "        for imagename in os.listdir(os.path.join(dir,label)):\n",
    "            image_paths.append(os.path.join(dir,label,imagename))\n",
    "            labels.append(label)\n",
    "        print(label, \"completed\")\n",
    "    return image_paths,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbfe8d10-425f-4ca0-a208-ae4e44444d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.DataFrame()\n",
    "train['image'], train['label'] = createdataframe(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a60368-b65b-4a59-aaf0-b1fc62b562a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   image     label\n",
      "0      C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "1      C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "2      C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "3      C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "4      C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "...                                                  ...       ...\n",
      "28816  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "28817  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "28818  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "28819  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "28820  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "\n",
      "[28821 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "950769a5-7397-47c2-a371-8573b8cfa169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry completed\n",
      "disgust completed\n",
      "fear completed\n",
      "happy completed\n",
      "neutral completed\n",
      "sad completed\n",
      "surprise completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = pd.DataFrame()\n",
    "test['image'], test['label'] = createdataframe(TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127a4da7-6ae2-4d40-b02c-62391ab12166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  image     label\n",
      "0     C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "1     C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "2     C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "3     C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "4     C:/Users/Asus/OneDrive/Documents/face recognit...     angry\n",
      "...                                                 ...       ...\n",
      "7061  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "7062  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "7063  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "7064  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "7065  C:/Users/Asus/OneDrive/Documents/face recognit...  surprise\n",
      "\n",
      "[7066 rows x 2 columns]\n",
      "0       C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "1       C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "2       C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "3       C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "4       C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "                              ...                        \n",
      "7061    C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "7062    C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "7063    C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "7064    C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "7065    C:/Users/Asus/OneDrive/Documents/face recognit...\n",
      "Name: image, Length: 7066, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bef8253-ff96-44b1-9d25-cee615441599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5c277d-6c53-4dc3-9222-6e4443636ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(images):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        img = load_img(image,grayscale =  True )\n",
    "        img = np.array(img)\n",
    "        features.append(img)\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(len(features),48,48,1)\n",
    "    return features\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a5f19-aa47-43ba-9244-b8dc413bb78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae69315f13a4435896f382cf7328427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28821 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\keras_preprocessing\\image\\utils.py:107: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "train_features = extract_features(train['image']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532a3b8-2436-4b42-9361-cfe8d58bb34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = extract_features(test['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3bb20-dabb-46b9-9d27-03bc2fbd38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = train_features/255.0\n",
    "x_test = test_features/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71b180-3804-4570-847d-abffc64f2457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ea8e4-a512-4473-8672-b70ce6ae6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Encode labels as integers\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(train['label'])\n",
    "y_test = le.transform(test['label'])\n",
    "\n",
    "# Step 2: Convert to one-hot encoding (only once!)\n",
    "y_train = to_categorical(y_train, num_classes=7)\n",
    "y_test = to_categorical(y_test, num_classes=7)\n",
    "\n",
    "# Optional: Check shape\n",
    "print(\"y_train shape:\", y_train.shape)  # Should be (num_samples, 7)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47f650-4c94-491d-849c-737ba603f54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# ðŸ‘‡ Input Layer\n",
    "model.add(Input(shape=(48, 48, 1)))\n",
    "\n",
    "# ðŸ‘‡ Convolutional Layers\n",
    "model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(512, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# ðŸ‘‡ Fully Connected Layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# ðŸ‘‡ Output Layer\n",
    "model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec894e37-9148-4e0b-9ba4-9bac15faeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887d494-b076-4903-9391-d4e3bc5e621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, validation_data=(x_test, y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46841f6a-f25f-4b1d-a10b-1543b6974ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"emotiondetector.json\",'w') as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a0836c-66e3-45dc-9c64-00dbff3143be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb50b5-8b46-424e-a81f-a5bf5ec50fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open(\"emotiondetector.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(\"emotiondetector.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76f98b0-8d5f-4921-9736-6052e8054d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['angry','disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923ea349-77a6-4e34-8c7c-fbd142b85618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ef(image):\n",
    "    img = load_img(image,grayscale =  True )\n",
    "    feature = np.array(img)\n",
    "    feature = feature.reshape(1,48,48,1)\n",
    "    return feature/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30739a55-2785-4cd7-a677-03e1903bffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'images/train/sad/42.jpg'\n",
    "print(\"original image is of sad\")\n",
    "img = ef(image)\n",
    "pred = model.predict(img)\n",
    "pred_label = label[pred.argmax()]\n",
    "print(\"model prediction is \",pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea39c9-f5ee-44c2-9b89-57339930a6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46b5ca-bc8b-4310-a9ac-5e5ce3cf24ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4acfe-4b07-4a35-8628-2a93b4d20477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1928e51b-183c-4cbb-99e5-9ebf9017aa57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
